{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "# import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4115072189525448316\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\" #for GPU Support on MacBook\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDir = os.getcwd()\n",
    "datasetDir = currentDir + \"\\\\FilteredImages\\\\\"\n",
    "# datasetDir = currentDir + \"/FilteredImages2/\" # NEW SINGLE IMAGES DIRECTORY\n",
    "trainDir = os.path.join(datasetDir, \"train\")\n",
    "testDir = os.path.join(datasetDir, \"test\")\n",
    "validDir = os.path.join(datasetDir, \"valid\")\n",
    "y_dataDir = os.path.join(datasetDir, \"y_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFirst(val):\n",
    "    return val[0]\n",
    "\n",
    "def getBinary(dataFile):\n",
    "    listOfTraining = []\n",
    "    trainingHeader = []\n",
    "    with open(dataFile) as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "        reader2 = csv.reader(csvfile)\n",
    "        listOfTraining = list(reader2)\n",
    "        trainingHeader = listOfTraining[0]\n",
    "        listOfTraining.pop(0)\n",
    "#         listOfTraining = listOfTraining.sort(key = sortFirst, reverse = False)\n",
    "#         np.asarray(listofTraining, dtype=np.int32)\n",
    "#         return np.asarray(listofTraining, dtype=np.int32)\n",
    "    listOfTrainingBinary = []\n",
    "    for item in listOfTraining:\n",
    "        listOfTrainingBinary.append(item[1])\n",
    "    return np.asarray(listOfTrainingBinary, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "y_trainDir = os.path.join(y_dataDir, 'train_split_Depression_AVEC2017-edited.csv')\n",
    "# print(y_trainDir)\n",
    "y_train = getBinary(y_trainDir)\n",
    "y_testDir = os.path.join(y_dataDir, 'dev_split_Depression_AVEC2017.csv')\n",
    "# print(y_testDir)\n",
    "y_test = getBinary(y_testDir)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "\n",
    "# Y_train = np.asarray(y_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainDir = trainDir\n",
    "x_testDir = testDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hong ray\\\\Desktop\\\\FYP\\\\Aaron-Workspace\\\\FilteredImages\\\\train'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trainDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testSet = []\n",
    "# testFolders =[]\n",
    "# # trainingSet = []\n",
    "# trainingFolders = []\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "\n",
    "def getImagesDir(mainDirectory):\n",
    "    ImgDict = {}\n",
    "    ImgList = []\n",
    "    \n",
    "    for folder in os.listdir(mainDirectory):\n",
    "        theDir = os.path.join(mainDirectory, folder)\n",
    "        images = os.listdir(theDir)\n",
    "        listOfImgDir = []\n",
    "        for img in images:\n",
    "#             listOfImgDir.append(Image.open(os.path.join(theDir, img)))\n",
    "            listOfImgDir.append(os.path.join(theDir, img))\n",
    "#         print(listOfImgDir)\n",
    "#         print(images)\n",
    "        folderName = folder.split('_')\n",
    "        ImgDict[folderName[0]] = listOfImgDir\n",
    "        ImgList.append(listOfImgDir)\n",
    "    imgList = ImgList.sort(key = sortFirst, reverse = False)\n",
    "    return ImgList\n",
    "        \n",
    "        \n",
    "#         for img in images\n",
    "    \n",
    "# for folder in toProcessList:\n",
    "#     images = os.listdir(datasetDir + \"/\" + folder)\n",
    "#     folderName = folder.split('_')\n",
    "# #     print(folderName[0])\n",
    "#     if folderName[0] in listOfTrainingName:\n",
    "#         trainingFolders.append(datasetDir + \"/\" + folder)\n",
    "# #         print(folderName)\n",
    "#         index = listOfTrainingName.index(folderName[0])\n",
    "#         temp = listOfTraining[index]\n",
    "#         x_train.append(images)\n",
    "#         tempBList = []\n",
    "#         val = temp[1]\n",
    "#         tempBList.append(val)\n",
    "#         y_train.append(tempBList)\n",
    "#         #temp.append(images)\n",
    "#         #trainingSet.append(temp)\n",
    "#     else:\n",
    "#         testFolders.append(datasetDir + \"/\" + folder)\n",
    "#         testSet.append(images)\n",
    "\n",
    "trainingImagesDir = getImagesDir(trainDir)\n",
    "testImagesDir = getImagesDir(testDir)\n",
    "# np.array(trainingImages).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importImages(listOfImgDir):\n",
    "    myFolder = []\n",
    "    for folder in listOfImgDir:\n",
    "        myImages = []\n",
    "        for image in folder:\n",
    "            myImages.append(np.array(Image.open(image)))\n",
    "        myFolder.append(np.array(myImages))\n",
    "    return myFolder\n",
    "\n",
    "\n",
    "#First Array iterate through Folder, Second Array Iterate though Image in Folder\n",
    "trainingImages = importImages(trainingImagesDir)\n",
    "testImages = importImages(testImagesDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Num of Img, Height, Width\n",
    "print(len(trainingImages)) \n",
    "\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingImagesNP = np.array(trainingImages)\n",
    "testImagesNP = np.array(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainingImages.shape)\n",
    "# print(trainingImages[0])\n",
    "# print(trainingImagesNP[10].shape)\n",
    "# print(y_train)\n",
    "\n",
    "neg = []\n",
    "y_neg = []\n",
    "pos = []\n",
    "y_pos = []\n",
    "\n",
    "for x,y in zip(trainingImages, y_train):\n",
    "    if (y == 0):\n",
    "        for each in x:\n",
    "            neg.append(each)\n",
    "            y_neg.append(0)\n",
    "    else:\n",
    "        for each in x:\n",
    "            pos.append(each)\n",
    "            y_pos.append(1)\n",
    "\n",
    "X = pos + neg\n",
    "Y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test = []\n",
    "y_neg_test = []\n",
    "pos_test = []\n",
    "y_pos_test = []\n",
    "\n",
    "for x,y in zip(testImages, y_train):\n",
    "    if (y == 0):\n",
    "        for each in x:\n",
    "            neg_test.append(each)\n",
    "            y_neg_test.append(0)\n",
    "    else:\n",
    "        for each in x:\n",
    "            pos_test.append(each)\n",
    "            y_pos_test.append(1)\n",
    "\n",
    "X_test = pos_test + neg_test\n",
    "Y_test = y_pos_test + y_neg_test\n",
    "\n",
    "\n",
    "npX_test = np.array(X_test)\n",
    "npY_test = np.array(Y_test)\n",
    "newNPX_test = npX_test.reshape(npX_test.shape[0], 101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vImagesDir = getImagesDir(validDir)\n",
    "vImages = importImages(vImagesDir)\n",
    "vImagesNP = np.array(vImages)\n",
    "\n",
    "#TESTING DATA UNSEEN DATA\n",
    "v = []\n",
    "# y_neg_v = []\n",
    "# pos_v = []\n",
    "# y_pos_v = []\n",
    "\n",
    "for x in vImages:\n",
    "    for each in x:\n",
    "        v.append(each)\n",
    "\n",
    "# X_v = pos_v + neg_v\n",
    "# Y_v = y_pos_v + y_neg_v\n",
    "\n",
    "\n",
    "np_v = np.array(v)\n",
    "# npY_test = np.array(Y_test)\n",
    "newNPX_v = np_v.reshape(np_v.shape[0], 101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npX = np.array(X)\n",
    "npY = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npX.shape)\n",
    "print(npY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newNPX = npX.reshape(npX.shape[0], 101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newNPX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainingImagesNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (101, 1000, 3)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#numEpochs = 100\n",
    "numEpochs = 25\n",
    "model_path = os.path.join(currentDir,'DAM-DHM-V-'+ str(numEpochs) +'.h5')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR HARRY\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def evaluate_model(X_train, X_val, y_train, y_val):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape, activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "    model.add(Conv2D(96, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(96, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta', #adam\n",
    "                  metrics=['accuracy']) \n",
    "    \n",
    "    print(model.metrics_names)\n",
    "    \n",
    "    model.save_weights('model.h5')\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_acc', patience = 10)]\n",
    "    \n",
    "    hist=model.fit(x=X_train, y=y_train, batch_size=32, epochs=numEpochs, callbacks=callbacks_list, validation_data=(X_val, y_val))\n",
    "    \n",
    "    _, val_acc=model.evaluate(x=X_val, y=y_val, verbose=1)\n",
    "  \n",
    "    model.load_weights('model.h5')\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    print(\"acc: \", np.mean(hist.history['acc']))\n",
    "    \n",
    "    print(\"val_acc: \", val_acc)\n",
    "    \n",
    "    model.save(''+ str(numEpochs) +' Epoch (Dr Harry Model) w validation.h5')\n",
    "    \n",
    "    plt.plot(hist.history['acc'])\n",
    "    plt.plot(hist.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    return model, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run CNN model\n",
    "model, val_acc = evaluate_model(newNPX, newNPX_test, npY, npY_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualisation(model):\n",
    "    layer_outputs = [layer.output for layer in model.layers[:1]] # Extracts the outputs of the top 12 layers\n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n",
    "    activations = activation_model.predict(newNPX_v) # Returns a list of five Numpy arrays: one array per layer activation\n",
    "    first_layer_activation = activations[0]\n",
    "    print(first_layer_activation.shape)\n",
    "    plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\n",
    "    layer_names = []\n",
    "    for layer in model.layers[:1]:\n",
    "        layer_names.append(model.name) # Names of the layers, so you can have them as part of your plot\n",
    "\n",
    "    images_per_row = 16\n",
    "    \n",
    "    for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "        n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
    "        size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "        n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0,\n",
    "                                                 :, :,\n",
    "                                                 col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "\n",
    "from keras import models       \n",
    "from keras.models import load_model\n",
    "model = load_model('30 Epoch (Dr Harry Model) w validation.h5')      \n",
    "visualisation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation, k = n_folds\n",
    "\n",
    "n_folds = 4\n",
    "count = 0\n",
    "cv_scores, model_history = list(), list()\n",
    "for _ in range(n_folds):\n",
    "    # split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(newNPX, npY, test_size=0.10, random_state = np.random.randint(1,1000, 1)[0])\n",
    "    # evaluate model\n",
    "    model, test_acc = evaluate_model(X_train, X_val, y_train, y_val)\n",
    "    count += 1\n",
    "    cv_scores.append(test_acc)\n",
    "    model_history.append(model)\n",
    "    print('K-Fold has ran ', count, ' time(s)')\n",
    "    \n",
    "print('\\nModel Accuracy after all K-Fold: ', (np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(newNPX_v)\n",
    "for each in result:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
