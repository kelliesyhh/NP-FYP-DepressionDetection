{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "# import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\" #for GPU Support on MacBook\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDir = os.getcwd()\n",
    "datasetDir = currentDir + \"\\\\FilteredImages\\\\\"\n",
    "# datasetDir = currentDir + \"/FilteredImages2/\" # NEW SINGLE IMAGES DIRECTORY\n",
    "trainDir = os.path.join(datasetDir, \"train\")\n",
    "testDir = os.path.join(datasetDir, \"test\")\n",
    "validDir = os.path.join(datasetDir, \"valid\")\n",
    "y_dataDir = os.path.join(datasetDir, \"y_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFirst(val):\n",
    "    return val[0]\n",
    "\n",
    "def getBinary(dataFile):\n",
    "    listOfTraining = []\n",
    "    trainingHeader = []\n",
    "    with open(dataFile) as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "        reader2 = csv.reader(csvfile)\n",
    "        listOfTraining = list(reader2)\n",
    "        trainingHeader = listOfTraining[0]\n",
    "        listOfTraining.pop(0)\n",
    "#         listOfTraining = listOfTraining.sort(key = sortFirst, reverse = False)\n",
    "#         np.asarray(listofTraining, dtype=np.int32)\n",
    "#         return np.asarray(listofTraining, dtype=np.int32)\n",
    "    listOfTrainingBinary = []\n",
    "    for item in listOfTraining:\n",
    "        listOfTrainingBinary.append(item[1])\n",
    "    return np.asarray(listOfTrainingBinary, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "y_trainDir = os.path.join(y_dataDir, 'train_split_Depression_AVEC2017-edited.csv')\n",
    "# print(y_trainDir)\n",
    "y_train = getBinary(y_trainDir)\n",
    "y_testDir = os.path.join(y_dataDir, 'dev_split_Depression_AVEC2017.csv')\n",
    "# print(y_testDir)\n",
    "y_test = getBinary(y_testDir)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "\n",
    "# Y_train = np.asarray(y_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainDir = trainDir\n",
    "x_testDir = testDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testSet = []\n",
    "# testFolders =[]\n",
    "# # trainingSet = []\n",
    "# trainingFolders = []\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "\n",
    "def getImagesDir(mainDirectory):\n",
    "    ImgDict = {}\n",
    "    ImgList = []\n",
    "    \n",
    "    for folder in os.listdir(mainDirectory):\n",
    "        theDir = os.path.join(mainDirectory, folder)\n",
    "        images = os.listdir(theDir)\n",
    "        listOfImgDir = []\n",
    "        for img in images:\n",
    "#             listOfImgDir.append(Image.open(os.path.join(theDir, img)))\n",
    "            listOfImgDir.append(os.path.join(theDir, img))\n",
    "#         print(listOfImgDir)\n",
    "#         print(images)\n",
    "        folderName = folder.split('_')\n",
    "        ImgDict[folderName[0]] = listOfImgDir\n",
    "        ImgList.append(listOfImgDir)\n",
    "    imgList = ImgList.sort(key = sortFirst, reverse = False)\n",
    "    return ImgList\n",
    "        \n",
    "        \n",
    "#         for img in images\n",
    "    \n",
    "# for folder in toProcessList:\n",
    "#     images = os.listdir(datasetDir + \"/\" + folder)\n",
    "#     folderName = folder.split('_')\n",
    "# #     print(folderName[0])\n",
    "#     if folderName[0] in listOfTrainingName:\n",
    "#         trainingFolders.append(datasetDir + \"/\" + folder)\n",
    "# #         print(folderName)\n",
    "#         index = listOfTrainingName.index(folderName[0])\n",
    "#         temp = listOfTraining[index]\n",
    "#         x_train.append(images)\n",
    "#         tempBList = []\n",
    "#         val = temp[1]\n",
    "#         tempBList.append(val)\n",
    "#         y_train.append(tempBList)\n",
    "#         #temp.append(images)\n",
    "#         #trainingSet.append(temp)\n",
    "#     else:\n",
    "#         testFolders.append(datasetDir + \"/\" + folder)\n",
    "#         testSet.append(images)\n",
    "\n",
    "trainingImagesDir = getImagesDir(trainDir)\n",
    "testImagesDir = getImagesDir(testDir)\n",
    "# np.array(trainingImages).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importImages(listOfImgDir):\n",
    "    myFolder = []\n",
    "    for folder in listOfImgDir:\n",
    "        myImages = []\n",
    "        for image in folder:\n",
    "            myImages.append(np.array(Image.open(image)))\n",
    "        myFolder.append(np.array(myImages))\n",
    "    return myFolder\n",
    "\n",
    "\n",
    "#First Array iterate through Folder, Second Array Iterate though Image in Folder\n",
    "trainingImages = importImages(trainingImagesDir)\n",
    "testImages = importImages(testImagesDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num of Img, Height, Width\n",
    "print(len(trainingImages)) \n",
    "\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingImagesNP = np.array(trainingImages)\n",
    "testImagesNP = np.array(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainingImages.shape)\n",
    "# print(trainingImages[0])\n",
    "# print(trainingImagesNP[10].shape)\n",
    "# print(y_train)\n",
    "\n",
    "neg = []\n",
    "y_neg = []\n",
    "pos = []\n",
    "y_pos = []\n",
    "\n",
    "for x,y in zip(trainingImages, y_train):\n",
    "    if (y == 0):\n",
    "        for each in x:\n",
    "            neg.append(each)\n",
    "            y_neg.append(0)\n",
    "    else:\n",
    "        for each in x:\n",
    "            pos.append(each)\n",
    "            y_pos.append(1)\n",
    "\n",
    "X = pos + neg\n",
    "Y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test = []\n",
    "y_neg_test = []\n",
    "pos_test = []\n",
    "y_pos_test = []\n",
    "\n",
    "for x,y in zip(testImages, y_train):\n",
    "    if (y == 0):\n",
    "        for each in x:\n",
    "            neg_test.append(each)\n",
    "            y_neg_test.append(0)\n",
    "    else:\n",
    "        for each in x:\n",
    "            pos_test.append(each)\n",
    "            y_pos_test.append(1)\n",
    "\n",
    "X_test = pos_test + neg_test\n",
    "Y_test = y_pos_test + y_neg_test\n",
    "\n",
    "\n",
    "npX_test = np.array(X_test)\n",
    "npY_test = np.array(Y_test)\n",
    "newNPX_test = npX_test.reshape(npX_test.shape[0], 101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vImagesDir = getImagesDir(validDir)\n",
    "vImages = importImages(vImagesDir)\n",
    "vImagesNP = np.array(vImages)\n",
    "\n",
    "#TESTING DATA UNSEEN DATA\n",
    "v = []\n",
    "# y_neg_v = []\n",
    "# pos_v = []\n",
    "# y_pos_v = []\n",
    "\n",
    "for x in vImages:\n",
    "    for each in x:\n",
    "        v.append(each)\n",
    "\n",
    "# X_v = pos_v + neg_v\n",
    "# Y_v = y_pos_v + y_neg_v\n",
    "\n",
    "\n",
    "np_v = np.array(v)\n",
    "# npY_test = np.array(Y_test)\n",
    "newNPX_v = np_v.reshape(np_v.shape[0], 101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npX = np.array(X)\n",
    "npY = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npX.shape)\n",
    "print(npY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newNPX = npX.reshape(npX.shape[0], 101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newNPX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainingImagesNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (101, 1000, 3)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#numEpochs = 100\n",
    "numEpochs = 30\n",
    "model_path = os.path.join(currentDir,'DAM-DHM-V-'+ str(numEpochs) +'.h5')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (101, 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR HARRY\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def evaluate_model(X_train, X_val, y_train, y_val):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape, activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "    model.add(Conv2D(96, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(96, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta', #adam\n",
    "                  metrics=['accuracy']) \n",
    "    \n",
    "    print(model.metrics_names)\n",
    "    \n",
    "    model.save_weights('model.h5')\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_acc', patience = 10)]\n",
    "    \n",
    "    hist=model.fit(x=X_train, y=y_train, batch_size=32, epochs=numEpochs, callbacks=callbacks_list, validation_data=(X_val, y_val))\n",
    "    \n",
    "    _, val_acc=model.evaluate(x=X_val, y=y_val, verbose=1)\n",
    "  \n",
    "    model.load_weights('model.h5')\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    print(\"acc: \", np.mean(hist.history['acc']))\n",
    "    \n",
    "    print(\"val_acc: \", val_acc)\n",
    "    \n",
    "    model.save(''+ str(numEpochs) +' Epoch (Dr Harry Model) w validation.h5')\n",
    "    \n",
    "    plt.plot(hist.history['acc'])\n",
    "    plt.plot(hist.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run CNN model\n",
    "model, val_acc = evaluate_model(newNPX, newNPX_test, npY, npY_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation, k = n_folds\n",
    "\n",
    "n_folds = 5\n",
    "count = 0\n",
    "cv_scores, model_history = list(), list()\n",
    "for _ in range(n_folds):\n",
    "    # split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(newNPX, npY, test_size=0.10, random_state = np.random.randint(1,1000, 1)[0])\n",
    "    # evaluate model\n",
    "    model, test_acc = evaluate_model(X_train, X_val, y_train, y_val)\n",
    "    count += 1\n",
    "    cv_scores.append(test_acc)\n",
    "    model_history.append(model)\n",
    "    print('K-Fold has ran ', count, ' time(s)')\n",
    "    \n",
    "print('\\nModel Accuracy after all K-Fold: ', (np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(newNPX_v)\n",
    "for each in result:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
