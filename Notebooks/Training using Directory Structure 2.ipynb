{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "# import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\" #for GPU Support on MacBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDir = os.getcwd()\n",
    "datasetDir = currentDir + \"/FilteredImages/\"\n",
    "datasetDir = currentDir + \"/FilteredImages2/\" # NEW SINGLE IMAGES DIRECTORY\n",
    "trainDir = os.path.join(datasetDir, \"train\")\n",
    "testDir = os.path.join(datasetDir, \"test\")\n",
    "validDir = os.path.join(datasetDir, \"valid\")\n",
    "y_dataDir = os.path.join(datasetDir, \"y_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFirst(val):\n",
    "    return val[0]\n",
    "\n",
    "def getBinary(dataFile):\n",
    "    listOfTraining = []\n",
    "    trainingHeader = []\n",
    "    with open(dataFile) as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "        reader2 = csv.reader(csvfile)\n",
    "        listOfTraining = list(reader2)\n",
    "        trainingHeader = listOfTraining[0]\n",
    "        listOfTraining.pop(0)\n",
    "#         listOfTraining = listOfTraining.sort(key = sortFirst, reverse = False)\n",
    "#         np.asarray(listofTraining, dtype=np.int32)\n",
    "#         return np.asarray(listofTraining, dtype=np.int32)\n",
    "    listOfTrainingBinary = []\n",
    "    for item in listOfTraining:\n",
    "        listOfTrainingBinary.append(item[1])\n",
    "    return np.asarray(listOfTrainingBinary, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "y_trainDir = os.path.join(y_dataDir, 'train_split_Depression_AVEC2017-edited.csv')\n",
    "# print(y_trainDir)\n",
    "y_train = getBinary(y_trainDir)\n",
    "y_testDir = os.path.join(y_dataDir, 'dev_split_Depression_AVEC2017.csv')\n",
    "# print(y_testDir)\n",
    "y_test = getBinary(y_testDir)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "\n",
    "# Y_train = np.asarray(y_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainDir = trainDir\n",
    "x_testDir = testDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/workspace/Aaron-Workspace/FilteredImages2/train'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trainDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (101, 1000, 3)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN FROM DEPRESSION DETECT\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', strides=1,\n",
    "                 input_shape=input_shape, activation='relu'))\n",
    "\n",
    "# model.add(GlobalAveragePooling2D(data_format='channels_last'))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(4, 3), strides=(1, 3)))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 3)))\n",
    "model.add(Conv2D(32, (1, 3), padding='valid', strides=1,\n",
    "          input_shape=input_shape, activation='relu'))\n",
    "\n",
    "#model.add(GlobalAveragePooling2D(data_format=(32, (101, 1000, 3))\n",
    "model.add(GlobalAveragePooling2D(data_format='channels_last'))\n",
    "#model.add(MaxPooling2D(pool_size=(1, 3), strides=(1, 3)))\n",
    "\n",
    "#model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "# model.add(Activation(tf.nn.softmax)) #fixes axis error for softmax\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#              optimizer='adadelta',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adadelta',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop', #DO NOT USE RSMPROP VERY BAD OPTIMISATION\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adadelta', #adam\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 107 images belonging to 107 classes.\n",
      "Found 35 images belonging to 35 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "\n",
    "# train_datagen = keras.utils.Sequence()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        x_trainDir,\n",
    "        target_size=(101, 1000),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        x_testDir,\n",
    "        target_size=(101, 1000),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes the class values generated from the generator\n",
    "def updateBinary(generator, y_binary):\n",
    "    counter = 0\n",
    "    for k,v in generator.class_indices.items():\n",
    "        generator.class_indices[k] = y_binary[counter]\n",
    "        counter += 1\n",
    "\n",
    "# counter = 0\n",
    "# for k,v in train_generator.class_indices.items():\n",
    "#     train_generator.class_indices[k] = y_train[counter]\n",
    "#     counter += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in train_generator.class_indices.items():\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('303_AUDIO_Participant', 0)\n",
      "('304_AUDIO_Participant', 1)\n",
      "('305_AUDIO_Participant', 2)\n",
      "('310_AUDIO_Participant', 3)\n",
      "('312_AUDIO_Participant', 4)\n",
      "('313_AUDIO_Participant', 5)\n",
      "('315_AUDIO_Participant', 6)\n",
      "('316_AUDIO_Participant', 7)\n",
      "('317_AUDIO_Participant', 8)\n",
      "('318_AUDIO_Participant', 9)\n",
      "('319_AUDIO_Participant', 10)\n",
      "('320_AUDIO_Participant', 11)\n",
      "('321_AUDIO_Participant', 12)\n",
      "('322_AUDIO_Participant', 13)\n",
      "('324_AUDIO_Participant', 14)\n",
      "('325_AUDIO_Participant', 15)\n",
      "('326_AUDIO_Participant', 16)\n",
      "('327_AUDIO_Participant', 17)\n",
      "('328_AUDIO_Participant', 18)\n",
      "('330_AUDIO_Participant', 19)\n",
      "('333_AUDIO_Participant', 20)\n",
      "('336_AUDIO_Participant', 21)\n",
      "('338_AUDIO_Participant', 22)\n",
      "('339_AUDIO_Participant', 23)\n",
      "('340_AUDIO_Participant', 24)\n",
      "('341_AUDIO_Participant', 25)\n",
      "('343_AUDIO_Participant', 26)\n",
      "('344_AUDIO_Participant', 27)\n",
      "('345_AUDIO_Participant', 28)\n",
      "('347_AUDIO_Participant', 29)\n",
      "('348_AUDIO_Participant', 30)\n",
      "('350_AUDIO_Participant', 31)\n",
      "('351_AUDIO_Participant', 32)\n",
      "('352_AUDIO_Participant', 33)\n",
      "('353_AUDIO_Participant', 34)\n",
      "('355_AUDIO_Participant', 35)\n",
      "('356_AUDIO_Participant', 36)\n",
      "('357_AUDIO_Participant', 37)\n",
      "('358_AUDIO_Participant', 38)\n",
      "('360_AUDIO_Participant', 39)\n",
      "('362_AUDIO_Participant', 40)\n",
      "('363_AUDIO_Participant', 41)\n",
      "('364_AUDIO_Participant', 42)\n",
      "('366_AUDIO_Participant', 43)\n",
      "('368_AUDIO_Participant', 44)\n",
      "('369_AUDIO_Participant', 45)\n",
      "('370_AUDIO_Participant', 46)\n",
      "('371_AUDIO_Participant', 47)\n",
      "('372_AUDIO_Participant', 48)\n",
      "('374_AUDIO_Participant', 49)\n",
      "('375_AUDIO_Participant', 50)\n",
      "('376_AUDIO_Participant', 51)\n",
      "('379_AUDIO_Participant', 52)\n",
      "('380_AUDIO_Participant', 53)\n",
      "('383_AUDIO_Participant', 54)\n",
      "('385_AUDIO_Participant', 55)\n",
      "('386_AUDIO_Participant', 56)\n",
      "('391_AUDIO_Participant', 57)\n",
      "('392_AUDIO_Participant', 58)\n",
      "('393_AUDIO_Participant', 59)\n",
      "('397_AUDIO_Participant', 60)\n",
      "('400_AUDIO_Participant', 61)\n",
      "('401_AUDIO_Participant', 62)\n",
      "('402_AUDIO_Participant', 63)\n",
      "('409_AUDIO_Participant', 64)\n",
      "('412_AUDIO_Participant', 65)\n",
      "('414_AUDIO_Participant', 66)\n",
      "('415_AUDIO_Participant', 67)\n",
      "('416_AUDIO_Participant', 68)\n",
      "('419_AUDIO_Participant', 69)\n",
      "('423_AUDIO_Participant', 70)\n",
      "('425_AUDIO_Participant', 71)\n",
      "('426_AUDIO_Participant', 72)\n",
      "('427_AUDIO_Participant', 73)\n",
      "('428_AUDIO_Participant', 74)\n",
      "('429_AUDIO_Participant', 75)\n",
      "('430_AUDIO_Participant', 76)\n",
      "('433_AUDIO_Participant', 77)\n",
      "('434_AUDIO_Participant', 78)\n",
      "('437_AUDIO_Participant', 79)\n",
      "('441_AUDIO_Participant', 80)\n",
      "('443_AUDIO_Participant', 81)\n",
      "('444_AUDIO_Participant', 82)\n",
      "('445_AUDIO_Participant', 83)\n",
      "('446_AUDIO_Participant', 84)\n",
      "('447_AUDIO_Participant', 85)\n",
      "('448_AUDIO_Participant', 86)\n",
      "('449_AUDIO_Participant', 87)\n",
      "('454_AUDIO_Participant', 88)\n",
      "('455_AUDIO_Participant', 89)\n",
      "('456_AUDIO_Participant', 90)\n",
      "('457_AUDIO_Participant', 91)\n",
      "('459_AUDIO_Participant', 92)\n",
      "('463_AUDIO_Participant', 93)\n",
      "('464_AUDIO_Participant', 94)\n",
      "('468_AUDIO_Participant', 95)\n",
      "('471_AUDIO_Participant', 96)\n",
      "('473_AUDIO_Participant', 97)\n",
      "('474_AUDIO_Participant', 98)\n",
      "('475_AUDIO_Participant', 99)\n",
      "('478_AUDIO_Participant', 100)\n",
      "('479_AUDIO_Participant', 101)\n",
      "('485_AUDIO_Participant', 102)\n",
      "('486_AUDIO_Participant', 103)\n",
      "('487_AUDIO_Participant', 104)\n",
      "('488_AUDIO_Participant', 105)\n",
      "('491_AUDIO_Participant', 106)\n"
     ]
    }
   ],
   "source": [
    "for item in train_generator.class_indices.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates each folder with the corresponding binary for depression/non-depression cases\n",
    "updateBinary(train_generator, y_train)\n",
    "updateBinary(validation_generator, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in train_generator.class_indices.items():\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_path = os.path.join(currentDir,'DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_135 (Conv2D)          (None, 101, 1000, 32)     896       \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 101, 1000, 32)     9248      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_25 (Averag (None, 25, 333, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_137 (Conv2D)          (None, 25, 333, 64)       18496     \n",
      "_________________________________________________________________\n",
      "conv2d_138 (Conv2D)          (None, 25, 333, 64)       36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_26 (Averag (None, 6, 111, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_139 (Conv2D)          (None, 6, 111, 96)        55392     \n",
      "_________________________________________________________________\n",
      "conv2d_140 (Conv2D)          (None, 6, 111, 96)        83040     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_27 (Averag (None, 1, 37, 96)         0         \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 3552)              0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 256)               909568    \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,179,617\n",
      "Trainable params: 1,179,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DR HARRY\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape, activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "model.add(Conv2D(96, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(96, (3, 3), padding='same', activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(4, 3)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta', #adam\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 109s 218ms/step - loss: -826.5725 - acc: 0.0097\n",
      "\n",
      "Epoch 00001: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 108s 215ms/step - loss: -832.2769 - acc: 0.0091\n",
      "\n",
      "Epoch 00002: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 108s 216ms/step - loss: -830.7487 - acc: 0.0095\n",
      "\n",
      "Epoch 00003: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 108s 216ms/step - loss: -826.0026 - acc: 0.0094\n",
      "\n",
      "Epoch 00004: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 108s 216ms/step - loss: -828.2200 - acc: 0.0091\n",
      "\n",
      "Epoch 00005: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f949b86bb38>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=2000,\n",
    "#         epochs=50,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=800)\n",
    "\n",
    "numOfSteps = 500\n",
    "numOfEpoch = 5\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=numOfSteps,\n",
    "        callbacks=callbacks_list,\n",
    "#         batch_size=batchSize\n",
    "        epochs=numOfEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"50Epoch (NEW SINGLE IMAGE DIR)2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.fit_generator(\n",
    "# #         train_generator,\n",
    "# #         steps_per_epoch=2000,\n",
    "# #         epochs=50,\n",
    "# #         validation_data=validation_generator,\n",
    "# #         validation_steps=800)\n",
    "\n",
    "# numOfSteps = 500\n",
    "# numOfEpoch = 30\n",
    "# numOfValidation = 100\n",
    "# batchSize = 32\n",
    "\n",
    "# # model.fit_generator(\n",
    "# #         train_generator,\n",
    "# #         steps_per_epoch=numOfSteps,\n",
    "# #         callbacks=callbacks_list,\n",
    "# # #         batch_size=batchSize\n",
    "# #         epochs=numOfEpoch,\n",
    "# #         validation_data=validation_generator,\n",
    "# #         validation_steps=numOfValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# model.save(os.path.join(currentDir,'DepressionAnalysisModel2 after 7 epoch.h5'))  # creates a HDF5 file\n",
    "# #del model  # deletes the existing model\n",
    "\n",
    "# # returns a compiled model\n",
    "# # identical to the previous one\n",
    "# #model = load_model(os.join.path(currentDir,'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
