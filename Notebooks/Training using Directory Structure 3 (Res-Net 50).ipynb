{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "# import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.applications import resnet50\n",
    "#Load the ResNet50 model\n",
    "# resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    "# resnet_model = resnet50.ResNet50(weights='None')\n",
    "resnet_model = resnet50.ResNet50(weights= None, include_top=False, input_shape=(101,1000,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/workspace/Aaron-Workspace/FilteredImages2/train/415_AUDIO_Participant/415_AUDIO_Participant(0-80000).png\n"
     ]
    }
   ],
   "source": [
    "filename = 'images/cat.jpg'\n",
    "# load an image in PIL format\n",
    "# original_image = load_img(filename, target_size=(224, 224))\n",
    "FstImgDir = os.path.join(trainDir, os.listdir(trainDir)[0])\n",
    "singleImg = os.path.join(FstImgDir, os.listdir(FstImgDir)[0])\n",
    "print(singleImg)\n",
    "training_image = load_img(singleImg, target_size=(224,224)) \n",
    "# training_image = load_img(singleImg, target_size=(100,1000)) # IMAGE DIMENSIONS NOT RECOGNISED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the PIL image (width, height) to a NumPy array (height, width, channel)\n",
    "numpy_image = img_to_array(training_image)\n",
    "# print(numpy_image)\n",
    "input_image = np.expand_dims(numpy_image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL image size =  (1000, 100)\n",
      "NumPy image size =  (100, 1000, 3)\n",
      "Input image size =  (1, 100, 1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4708a1ff28>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('PIL image size = ', training_image.size)\n",
    "print('NumPy image size = ', numpy_image.shape)\n",
    "print('Input image size = ', input_image.shape)\n",
    "plt.imshow(np.uint8(input_image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image_resnet50 = resnet50.preprocess_input(input_image.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (224, 224, 3) but got array with shape (100, 1000, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-dfe965929d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions_resnet50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_image_resnet50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel_resnet50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_resnet50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'label_resnet50 = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_resnet50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (224, 224, 3) but got array with shape (100, 1000, 3)"
     ]
    }
   ],
   "source": [
    "predictions_resnet50 = resnet_model.predict(processed_image_resnet50)\n",
    "label_resnet50 = decode_predictions(predictions_resnet50)\n",
    "print ('label_resnet50 = ', label_resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\" #for GPU Support on MacBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDir = os.getcwd()\n",
    "datasetDir = currentDir + \"/FilteredImages/\"\n",
    "datasetDir = currentDir + \"/FilteredImages2/\" # NEW SINGLE IMAGES DIRECTORY\n",
    "trainDir = os.path.join(datasetDir, \"train\")\n",
    "testDir = os.path.join(datasetDir, \"test\")\n",
    "validDir = os.path.join(datasetDir, \"valid\")\n",
    "y_dataDir = os.path.join(datasetDir, \"y_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFirst(val):\n",
    "    return val[0]\n",
    "\n",
    "def getBinary(dataFile):\n",
    "    listOfTraining = []\n",
    "    trainingHeader = []\n",
    "    with open(dataFile) as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "        reader2 = csv.reader(csvfile)\n",
    "        listOfTraining = list(reader2)\n",
    "        trainingHeader = listOfTraining[0]\n",
    "        listOfTraining.pop(0)\n",
    "#         listOfTraining = listOfTraining.sort(key = sortFirst, reverse = False)\n",
    "#         np.asarray(listofTraining, dtype=np.int32)\n",
    "#         return np.asarray(listofTraining, dtype=np.int32)\n",
    "    listOfTrainingBinary = []\n",
    "    for item in listOfTraining:\n",
    "        listOfTrainingBinary.append(item[1])\n",
    "    return np.asarray(listOfTrainingBinary, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "y_trainDir = os.path.join(y_dataDir, 'train_split_Depression_AVEC2017-edited.csv')\n",
    "# print(y_trainDir)\n",
    "y_train = getBinary(y_trainDir)\n",
    "y_testDir = os.path.join(y_dataDir, 'dev_split_Depression_AVEC2017.csv')\n",
    "# print(y_testDir)\n",
    "y_test = getBinary(y_testDir)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "\n",
    "# Y_train = np.asarray(y_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainDir = trainDir\n",
    "x_testDir = testDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/workspace/Aaron-Workspace/FilteredImages2/train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trainDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 99, 998, 32)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 99, 996, 32)       3104      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               16896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 21,409\n",
      "Trainable params: 21,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (101, 1000, 3)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# TAKEN FROM DEPRESSION DETECT\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', strides=1,\n",
    "                 input_shape=input_shape, activation='relu'))\n",
    "\n",
    "# model.add(GlobalAveragePooling2D(data_format='channels_last'))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(4, 3), strides=(1, 3)))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 3)))\n",
    "model.add(Conv2D(32, (1, 3), padding='valid', strides=1,\n",
    "          input_shape=input_shape, activation='relu'))\n",
    "\n",
    "#model.add(GlobalAveragePooling2D(data_format=(32, (101, 1000, 3))\n",
    "model.add(GlobalAveragePooling2D(data_format='channels_last'))\n",
    "#model.add(MaxPooling2D(pool_size=(1, 3), strides=(1, 3)))\n",
    "\n",
    "#model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "# model.add(Activation(tf.nn.softmax)) #fixes axis error for softmax\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#              optimizer='adadelta',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adadelta',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop', #DO NOT USE RSMPROP VERY BAD OPTIMISATION\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adadelta', #adam\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 images belonging to 103 classes.\n",
      "Found 35 images belonging to 35 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "\n",
    "# train_datagen = keras.utils.Sequence()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        x_trainDir,\n",
    "        target_size=(101, 1000),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        x_testDir,\n",
    "        target_size=(101, 1000),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes the class values generated from the generator\n",
    "def updateBinary(generator, y_binary):\n",
    "    counter = 0\n",
    "    for k,v in generator.class_indices.items():\n",
    "        generator.class_indices[k] = y_binary[counter]\n",
    "        counter += 1\n",
    "\n",
    "# counter = 0\n",
    "# for k,v in train_generator.class_indices.items():\n",
    "#     train_generator.class_indices[k] = y_train[counter]\n",
    "#     counter += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in train_generator.class_indices.items():\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('302_AUDIO_Participant', 0)\n",
      "('307_AUDIO_Participant', 0)\n",
      "('331_AUDIO_Participant', 0)\n",
      "('335_AUDIO_Participant', 1)\n",
      "('346_AUDIO_Participant', 1)\n",
      "('367_AUDIO_Participant', 1)\n",
      "('377_AUDIO_Participant', 1)\n",
      "('381_AUDIO_Participant', 1)\n",
      "('382_AUDIO_Participant', 0)\n",
      "('388_AUDIO_Participant', 1)\n",
      "('389_AUDIO_Participant', 1)\n",
      "('390_AUDIO_Participant', 0)\n",
      "('395_AUDIO_Participant', 0)\n",
      "('403_AUDIO_Participant', 0)\n",
      "('404_AUDIO_Participant', 0)\n",
      "('406_AUDIO_Participant', 0)\n",
      "('413_AUDIO_Participant', 1)\n",
      "('417_AUDIO_Participant', 0)\n",
      "('418_AUDIO_Participant', 1)\n",
      "('420_AUDIO_Participant', 0)\n",
      "('422_AUDIO_Participant', 1)\n",
      "('436_AUDIO_Participant', 0)\n",
      "('439_AUDIO_Participant', 0)\n",
      "('440_AUDIO_Participant', 1)\n",
      "('451_AUDIO_Participant', 0)\n",
      "('458_AUDIO_Participant', 0)\n",
      "('472_AUDIO_Participant', 0)\n",
      "('476_AUDIO_Participant', 0)\n",
      "('477_AUDIO_Participant', 0)\n",
      "('482_AUDIO_Participant', 0)\n",
      "('483_AUDIO_Participant', 1)\n",
      "('484_AUDIO_Participant', 0)\n",
      "('489_AUDIO_Participant', 0)\n",
      "('490_AUDIO_Participant', 0)\n",
      "('492_AUDIO_Participant', 0)\n"
     ]
    }
   ],
   "source": [
    "for item in validation_generator.class_indices.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates each folder with the corresponding binary for depression/non-depression cases\n",
    "updateBinary(train_generator, y_train)\n",
    "updateBinary(validation_generator, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in train_generator.class_indices.items():\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_path = os.path.join(currentDir,'DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='loss', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='acc', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3381.5972 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00002: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3375.3307 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00003: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3370.7836 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00004: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3359.8405 - acc: 0.0096 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00005: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3359.7106 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00006: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3427.1320 - acc: 0.0096 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00007: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3395.4882 - acc: 0.0098 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00008: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3399.9026 - acc: 0.0109 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00009: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3378.7262 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00010: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3370.3668 - acc: 0.0103 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00011: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3374.0883 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00012: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3391.4320 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00013: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3360.3255 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00014: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3376.7500 - acc: 0.0107 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00015: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3373.3080 - acc: 0.0087 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00016: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3372.4866 - acc: 0.0103 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00017: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3366.5708 - acc: 0.0089 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00018: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3382.4313 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00019: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3369.6508 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00020: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3376.0189 - acc: 0.0098 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00021: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3381.4778 - acc: 0.0098 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00022: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3388.0620 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00023: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3378.1537 - acc: 0.0089 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00024: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3370.4256 - acc: 0.0083 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00025: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3394.6330 - acc: 0.0105 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00026: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3385.9200 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00027: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3355.6103 - acc: 0.0107 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00028: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3397.4840 - acc: 0.0112 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00029: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3405.7883 - acc: 0.0098 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00030: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3373.2151 - acc: 0.0107 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00031: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3388.7668 - acc: 0.0103 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00032: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3373.8696 - acc: 0.0103 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00033: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 73s 146ms/step - loss: 3349.3289 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00034: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3409.4968 - acc: 0.0096 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00035: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3340.5507 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00036: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3389.6015 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00037: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3380.2775 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00038: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3420.9282 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00039: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3388.7501 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00040: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3351.9732 - acc: 0.0092 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00041: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3366.6913 - acc: 0.0096 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00042: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3368.7337 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00043: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3401.6466 - acc: 0.0100 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00044: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3398.8094 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00045: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3412.5157 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00046: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3371.8141 - acc: 0.0096 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00047: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3404.7398 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00048: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3371.7136 - acc: 0.0098 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00049: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 73s 146ms/step - loss: 3402.4680 - acc: 0.0094 - val_loss: 358.0000 - val_acc: 0.0286\n",
      "\n",
      "Epoch 00050: saving model to /home/ec2-user/workspace/Aaron-Workspace/DepressionAnalysisModel5 (NEW SINGLE IMG)2.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa008175b00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=2000,\n",
    "#         epochs=50,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=800)\n",
    "\n",
    "numOfSteps = 500\n",
    "numOfEpoch = 50\n",
    "numOfValidation = 20\n",
    "batchSize = 32\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=numOfSteps,\n",
    "        callbacks=callbacks_list,\n",
    "#         batch_size=batchSize\n",
    "        epochs=numOfEpoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=numOfValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"50Epoch (NEW SINGLE IMAGE DIR)2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.fit_generator(\n",
    "# #         train_generator,\n",
    "# #         steps_per_epoch=2000,\n",
    "# #         epochs=50,\n",
    "# #         validation_data=validation_generator,\n",
    "# #         validation_steps=800)\n",
    "\n",
    "# numOfSteps = 500\n",
    "# numOfEpoch = 30\n",
    "# numOfValidation = 100\n",
    "# batchSize = 32\n",
    "\n",
    "# # model.fit_generator(\n",
    "# #         train_generator,\n",
    "# #         steps_per_epoch=numOfSteps,\n",
    "# #         callbacks=callbacks_list,\n",
    "# # #         batch_size=batchSize\n",
    "# #         epochs=numOfEpoch,\n",
    "# #         validation_data=validation_generator,\n",
    "# #         validation_steps=numOfValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# model.save(os.path.join(currentDir,'DepressionAnalysisModel2 after 7 epoch.h5'))  # creates a HDF5 file\n",
    "# #del model  # deletes the existing model\n",
    "\n",
    "# # returns a compiled model\n",
    "# # identical to the previous one\n",
    "# #model = load_model(os.join.path(currentDir,'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
